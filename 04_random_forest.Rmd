---
title: "Predictive modeling"
output:
  html_document:
    theme: spacelab
    highlight: tango
    mathjax: default
    toc: false
---

<style type="text/css">
:is(h1, h2, h3, h4) {
  color: #0B2949; font-family: "Montserrat", sans-serif; font-style: normal; font-weight: 600;
} 
body {
  color: #555555; font-family: "Montserrat", sans-serif; font-style: normal; font-weight: 400;
}
</style> 


# Random forest {.tabset .tabset-fade .tabset-pills .p-3 .border .rounded}  

## Setup {.tabset .tabset-fade .p-3 .rounded} 


```{r, echo = F}
knitr::opts_chunk$set(echo = T)
```

```{r}
# directories
root   <- paste0("C:/Users/", Sys.getenv("USERNAME"), "/Projects/portugal")

fn     <- file.path(root, "fn")
in_csv <- file.path(root, "src", "final", "student.csv")

# package declarations
suppressPackageStartupMessages({
  library(assertr)
  library(dplyr)
  library(ranger)
  library(readr)
  library(tidyr)
})

# options
options(lifecycle_disable_warnings = T)
options(dplyr.print_max = 100)
set.seed(29813)

# source functions
invisible(sapply(list.files(fn, pattern = "\\.R", full.names = T), source))
```


```{r}
# load data
df <- readr::read_csv(in_csv, show_col_types = F) %>%
      assertr::assert_rows(col_concat, is_uniq, subj, pseudo_id)

# model preparation
excl <- c("subj", "pseudo_id", "post_test", "post_test_gtp25", "post_test_gtp50", "post_test_gtp75")
predictors <- names(df)[!(names(df) %in% c(excl))]

outcomes <- c("post_test_gtp25", "post_test_gtp50", "post_test_gtp75")
```




## Baseline model {.tabset .tabset-fade .p-3 .rounded} 


```{r}
# use reading exam scores as training set
df.pred <- NULL
models  <- list()

seeds <- c(54622, 35685, 2323456)

for (o in c(outcomes)) {

  # model-specific data frame
  df.est <-
    df %>%
    filter(subj=="por") %>%
    drop_na(all_of(c(o))) %>%
    drop_na(all_of(c(predictors))) %>%
    mutate(!!sym(paste0(o,"_factor")) := as.factor(!!sym(o)==1))

  # baseline accuracy
  mu <- mean(df.est[[o]])

  # random forest
  seeds <- seeds[2:length(seeds)]

  rf.fit <-
    ranger::ranger(
      as.formula(paste(paste0(o, "_factor"), "~", paste0(c(predictors), collapse = " + "))),
      data      = df.est,
      num.trees = 500,
      mtry      = floor(sqrt(length(c(predictors)))),
      replace   = T,
      seed      = seeds[1])

  models[[o]] <- rf.fit

  print(rf.fit)

  rf.rates <-
    pred_rates(
      prediction = as.numeric(as.logical(rf.fit$predictions)==T),
      reference  = as.numeric(as.logical(df.est[[o]])==T))

  df.pred <-
    rbind(
      df.pred,
      tibble(
         set     = "train",
         y       = o,
         mu      = mu,
         rf_acc  = rf.rates$acc,
         rf_sens = rf.rates$sens,
         rf_spec = rf.rates$spec,
         rf_prec = rf.rates$prec
      )
    )

}
```


```{r}
# use math exam scores as test set
for (o in c(outcomes)) {

  # model-specific data frame
  df.est <-
    df %>%
    filter(subj=="mat") %>%
    drop_na(all_of(c(o))) %>%
    drop_na(all_of(c(predictors))) %>%
    mutate(!!sym(paste0(o,"_factor")) := as.factor(!!sym(o)==1))

  # baseline accuracy
  mu <- mean(df.est[[o]])

  # test model
  rf.test <- ranger:::predict.ranger(models[[o]], data = df.est)

  print(rf.test)

  rf.rates <-
    pred_rates(
      prediction = as.numeric(as.logical(rf.test$predictions)==T),
      reference  = as.numeric(as.logical(df.est[[o]])==T))

  df.pred <-
    rbind(
      df.pred,
      tibble(
         set     = "test",
         y       = o,
         mu      = mu,
         rf_acc  = rf.rates$acc,
         rf_sens = rf.rates$sens,
         rf_spec = rf.rates$spec,
         rf_prec = rf.rates$prec
      )
    )

}
```


```{r}
# assess train/test performance
df.pred

## Tuned model {.tabset .tabset-fade .p-3 .rounded} 


```{r}
# hyperparameter tuning (mtry)
df.tune <- NULL
models  <- list()

tc <- caret::trainControl(method = "repeatedcv", number = 5, repeats = 3, search = "grid")
  # 5-fold cross-validation, repeated 3x

mtry.base <- floor(sqrt(length(c(predictors))))
  # baseline mtry = number of predictors considers at each split

tg <- expand.grid(.mtry = floor(mtry.base/2):(mtry.base*3))
  # tune grid for mtry

for (o in c(outcomes)) {

  # model-specific data frame
  df.est <-
    df %>%
    filter(subj=="por") %>%
    drop_na(all_of(c(o))) %>%
    drop_na(all_of(c(predictors))) %>%
    mutate(!!sym(paste0(o,"_factor")) := as.factor(!!sym(o)==1))

  # train model
  rf.tune <-
    caret::train(
      as.formula(paste(paste0(o, "_factor"), "~", paste0(c(predictors), collapse = " + "))),
      data       = df.est,
      metric     = "Accuracy",
      trControl  = tc,
      tuneGrid   = tg
    )
  
  models[[o]] <- rf.tune
  
  df.tune <-
    rbind(
      df.tune,
      tibble(
        set    = "tune",
        y      = o,
        opt    = as.numeric(rf.tune$bestTune$mtry==rf.tune$results$mtry),
        mtry   = rf.tune$results$mtry,
        rf_acc = rf.tune$results$Accuracy
      )
    )

}
```


```{r}
# visualize results of mtry tuning
ggplot(
  df.tune,
  aes(y     = rf_acc,
      x     = mtry,
      color = as.factor(y))) +
  geom_line() +
  geom_point(
    size  = 2,
    shape = 17) +
  geom_point(
    data = df.tune %>% filter(opt==1),
    aes(y = rf_acc,
        x = mtry),
    size  = 5,
    shape = 19,
    show.legend = F) +
  scale_color_manual(
    values = c("blue", "green", "red")) +
  labs(y = "Accuracy", x = "mtry") +
  custom_theme
```


```{r}
# estimate tuned model on training set
df.pred <- NULL
models  <- list()

seeds <- c(295195, 482951, 819514)

for (o in c(outcomes)) {

  # model-specific data frame
  df.est <-
    df %>%
    filter(subj=="por") %>%
    drop_na(all_of(c(o))) %>%
    drop_na(all_of(c(predictors))) %>%
    mutate(!!sym(paste0(o,"_factor")) := as.factor(!!sym(o)==1))

  # baseline accuracy
  mu <- mean(df.est[[o]])

  # random forest
  seeds <- seeds[2:length(seeds)]

  rf.fit <-
    ranger::ranger(
      as.formula(paste(paste0(o, "_factor"), "~", paste0(c(predictors), collapse = " + "))),
      data      = df.est,
      num.trees = 500,
      mtry      = df.tune %>% filter(y==all_of(o) & opt==1) %>% pull(mtry),
      replace   = T,
      seed      = seeds[1])

  models[[o]] <- rf.fit

  rf.rates <-
    pred_rates(
      prediction = as.numeric(as.logical(rf.fit$predictions)==T),
      reference  = as.numeric(as.logical(df.est[[o]])==T))

  df.pred <-
    rbind(
      df.pred,
      tibble(
         set     = "train",
         y       = o,
         mu      = mu,
         rf_acc  = rf.rates$acc,
         rf_sens = rf.rates$sens,
         rf_spec = rf.rates$spec,
         rf_prec = rf.rates$prec
      )
    )

}
```


```{r}
# use math exam scores as test set
for (o in c(outcomes)) {

  # model-specific data frame
  df.est <-
    df %>%
    filter(subj=="mat") %>%
    drop_na(all_of(c(o))) %>%
    drop_na(all_of(c(predictors))) %>%
    mutate(!!sym(paste0(o,"_factor")) := as.factor(!!sym(o)==1))

  # baseline accuracy
  mu <- mean(df.est[[o]])

  # test model
  rf.test <- ranger:::predict.ranger(models[[o]], data = df.est)

  rf.rates <-
    pred_rates(
      prediction = as.numeric(as.logical(rf.test$predictions)==T),
      reference  = as.numeric(as.logical(df.est[[o]])==T))

  df.pred <-
    rbind(
      df.pred,
      tibble(
         set     = "test",
         y       = o,
         mu      = mu,
         rf_acc  = rf.rates$acc,
         rf_sens = rf.rates$sens,
         rf_spec = rf.rates$spec,
         rf_prec = rf.rates$prec
      )
    )

}
